{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Dense, Flatten, Conv2D, Lambda, Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the FER2013 dataset.\n",
    "    \"\"\"\n",
    "file_path = '/kaggle/input/fer2013/fer2013.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "pixels = data['pixels'].tolist()\n",
    "width, height = 48, 48\n",
    "faces = []\n",
    "for pixel_sequence in pixels:\n",
    "    face = np.asarray([int(pixel) for pixel in pixel_sequence.split(' ')]).reshape(width, height)\n",
    "    face = np.stack((face,)*3, axis=-1)  # Convert to RGB\n",
    "    faces.append(face)\n",
    "faces = np.array(faces)\n",
    "faces = faces.astype('float32') / 255.0\n",
    "\n",
    "emotions = pd.get_dummies(data['emotion']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(faces))\n",
    "input_train, input_test = faces[:train_size], faces[train_size:]\n",
    "target_train, target_test = emotions[:train_size], emotions[train_size:]\n",
    "\n",
    "#     return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# (input_train, target_train), (input_test, target_test) = (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "input_train = X_train\n",
    "target_train = y_train\n",
    "input_test = X_test\n",
    "target_test = y_test\n",
    "# print(\"\",input_train.shape)\n",
    "\n",
    "width, height, dim = 48, 48, 3\n",
    "num_classes = 7\n",
    "# def preprocess_image(image):\n",
    "#     image = tf.image.resize(image, (48, 48))\n",
    "#     return image\n",
    "\n",
    "# Data augmentation: perform zero padding on datasets\n",
    "# paddings = tf.constant([[0, 0], [4, 4], [4, 4], [0, 0]])\n",
    "# input_train = tf.pad(input_train, paddings, mode=\"CONSTANT\")\n",
    "\n",
    "# Data generator for training data with additional augmentations\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10,          # Random rotations\n",
    "    width_shift_range=0.1,      # Random horizontal shifts\n",
    "    height_shift_range=0.1,     # Random vertical shifts\n",
    "    zoom_range=0.1,             # Random zoom\n",
    "    shear_range=0.1,            # Random shear\n",
    "    rescale=1./255\n",
    "#     preprocessing_function=preprocess_image\n",
    ")\n",
    "\n",
    "# Generate training and validation batches\n",
    "train_batches = train_generator.flow(input_train, target_train, batch_size=32, subset=\"training\")\n",
    "validation_batches = train_generator.flow(input_train, target_train, batch_size=32, subset=\"validation\")\n",
    "# batch_x, batch_y = next(train_batches)\n",
    "# print(batch_x.shape)\n",
    "# Data generator for testing data\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Generate test batches\n",
    "test_batches = test_generator.flow(input_test, target_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, random_crop_size):\n",
    "    \"\"\"\n",
    "    Perform a random crop on the image.\n",
    "    \"\"\"\n",
    "    height, width = img.shape[:2]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"\n",
    "    Generate random crops from image batches.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        num_channels = batch_x.shape[-1]  # Handle grayscale and RGB images\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, num_channels))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def residual_block(x, number_of_filters, match_filter_size=False):\n",
    "    \"\"\"\n",
    "    Residual block with convolutional layers.\n",
    "    \"\"\"\n",
    "#     config = model_configuration()\n",
    "    initializer = tf.keras.initializers.HeNormal() #config.get(\"initializer\")\n",
    "\n",
    "    # Create skip connection\n",
    "    x_skip = x\n",
    "\n",
    "    # Perform the original mapping\n",
    "    if match_filter_size:\n",
    "        x = Conv2D(number_of_filters, kernel_size=(3, 3), strides=(2,2), kernel_initializer=initializer, padding=\"same\")(x_skip)\n",
    "    else:\n",
    "        x = Conv2D(number_of_filters, kernel_size=(3, 3), strides=(1,1), kernel_initializer=initializer, padding=\"same\")(x_skip)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(number_of_filters, kernel_size=(3, 3), kernel_initializer=initializer, padding=\"same\")(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "\n",
    "    # Perform matching of filter numbers if necessary\n",
    "    if match_filter_size and config.get(\"shortcut_type\") == \"identity\":\n",
    "        x_skip = Lambda(lambda x: tf.pad(x[:, ::2, ::2, :], tf.constant([[0, 0], [0, 0], [0, 0], [number_of_filters//4, number_of_filters//4]]), mode=\"CONSTANT\"))(x_skip)\n",
    "    elif match_filter_size and config.get(\"shortcut_type\") == \"projection\":\n",
    "        x_skip = Conv2D(number_of_filters, kernel_size=(1,1), kernel_initializer=initializer, strides=(2,2))(x_skip)\n",
    "\n",
    "    # Add the skip connection to the regular mapping\n",
    "    x = Add()([x, x_skip])\n",
    "\n",
    "    # Nonlinearly activate the result\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def ResidualBlocks(x):\n",
    "    \"\"\"\n",
    "    Set up the residual blocks.\n",
    "    \"\"\"\n",
    "#     config = model_configuration()\n",
    "\n",
    "    # Set initial filter size\n",
    "    filter_size = 16 #config.get(\"initial_num_feature_maps\")\n",
    "\n",
    "    for layer_group in range(3):\n",
    "        for block in range(3): #config.get(\"stack_n\")):\n",
    "            if layer_group > 0 and block == 0:\n",
    "                filter_size *= 2\n",
    "                x = residual_block(x, filter_size, match_filter_size=True)\n",
    "            else:\n",
    "                x = residual_block(x, filter_size)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def model_base(shp):\n",
    "    \"\"\"\n",
    "    Base structure of the model, with residual blocks attached.\n",
    "    \"\"\"\n",
    "#     config = model_configuration()\n",
    "    initializer = tf.keras.initializers.HeNormal() #config.get(\"initializer\")\n",
    "\n",
    "    inputs = Input(shape=shp)\n",
    "    x = Conv2D(16, kernel_size=(3,3), strides=(1,1), kernel_initializer=initializer, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = ResidualBlocks(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     x = Flattenn()(x)\n",
    "    x = Dense(7, kernel_initializer=initializer)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def init_model():\n",
    "    \"\"\"\n",
    "    Initialize the model with the specified configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    optimizer_momentum = 0.9\n",
    "    optimizer_additional_metrics = [\"accuracy\"]\n",
    "    optimizer = SGD(learning_rate=lr_schedule, momentum=optimizer_momentum)\n",
    "#     config = model_configuration()\n",
    "#     width, height, dim = config.get(\"width\"), config.get(\"height\"), config.get(\"dim\")\n",
    "\n",
    "    model = model_base((width, height, dim))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=optimizer_additional_metrics)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# train_batches, validation_batches, _ = preprocessed_dataset()\n",
    "# train_batches = train_generator.flow(input_train, target_train, batch_size=32, subset=\"training\")\n",
    "# validation_batches\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=os.path.join(os.getcwd(), \"logs\"),\n",
    "    histogram_freq=1,\n",
    "    write_images=True\n",
    ")\n",
    "\n",
    "# Save a model checkpoint after every epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(os.getcwd(), \"model_checkpoint.keras\"),\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "# Add callbacks to list\n",
    "callbacks = [\n",
    "    tensorboard,\n",
    "    checkpoint\n",
    "]\n",
    "\n",
    "steps_per_epoch = int(np.floor(train_size / batch_size))\n",
    "val_steps_per_epoch = int(np.floor(val_size / batch_size))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "history = model.fit(\n",
    "    train_batches, \n",
    "    validation_data=validation_batches,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def load_dataset():\n",
    "    file_path = '/kaggle/input/fer2013/fer2013.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = np.asarray([int(pixel) for pixel in pixel_sequence.split(' ')]).reshape(width, height)\n",
    "        face = np.stack((face,)*3, axis=-1)  # Convert to RGB\n",
    "        faces.append(face)\n",
    "    faces = np.array(faces)\n",
    "    faces = faces.astype('float32') / 255.0\n",
    "\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(0.8 * len(faces))\n",
    "    input_train, input_test = faces[:train_size], faces[train_size:]\n",
    "    target_train, target_test = emotions[:train_size], emotions[train_size:]\n",
    "\n",
    "    return (input_train, target_train), (input_test, target_test)\n",
    "\n",
    "def residual_block(x, number_of_filters, match_filter_size=False):\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    x_skip = x\n",
    "    if match_filter_size:\n",
    "        x = Conv2D(number_of_filters, kernel_size=(3, 3), strides=(2,2), kernel_initializer=initializer, padding=\"same\")(x_skip)\n",
    "    else:\n",
    "        x = Conv2D(number_of_filters, kernel_size=(3, 3), strides=(1,1), kernel_initializer=initializer, padding=\"same\")(x_skip)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(number_of_filters, kernel_size=(3, 3), kernel_initializer=initializer, padding=\"same\")(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "\n",
    "    if match_filter_size:\n",
    "        x_skip = Conv2D(number_of_filters, kernel_size=(1,1), kernel_initializer=initializer, strides=(2,2))(x_skip)\n",
    "\n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def ResidualBlocks(x):\n",
    "    filter_size = 16\n",
    "    for layer_group in range(3):\n",
    "        for block in range(3):\n",
    "            if layer_group > 0 and block == 0:\n",
    "                filter_size *= 2\n",
    "                x = residual_block(x, filter_size, match_filter_size=True)\n",
    "            else:\n",
    "                x = residual_block(x, filter_size)\n",
    "    return x\n",
    "\n",
    "def model_base(shp):\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    inputs = Input(shape=shp)\n",
    "    x = Conv2D(16, kernel_size=(3,3), strides=(1,1), kernel_initializer=initializer, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = ResidualBlocks(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "    x = Dense(7, kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "def init_model():\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model = model_base((48, 48, 3))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "(input_train, target_train), (input_test, target_test) = load_dataset()\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_batches = train_generator.flow(input_train, target_train, batch_size=32, subset=\"training\")\n",
    "validation_batches = train_generator.flow(input_train, target_train, batch_size=32, subset=\"validation\")\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_batches = test_generator.flow(input_test, target_test, batch_size=32)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(os.getcwd(), \"logs\"), histogram_freq=1, write_images=True)\n",
    "checkpoint = ModelCheckpoint(os.path.join(os.getcwd(), \"model_checkpoint.keras\"), save_freq=\"epoch\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "callbacks = [tensorboard, checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "model = init_model()\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    validation_data=validation_batches,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
